\chapter{Second appendix}

\section{Expectation of Poisson data contributing to a measurement
\label{app:expected_a_b}}
%=========================
Assume the configuration of figure \pref{fig:expected_a_b}: two radioactive
sources contribute to a single measurement $N$. We know the a-priori expected
values $\bar{a}$ and $\bar{b}$ for each of the sources, and we know the
measured count $N = a + b$. The question is to compute the expected values of
$a$ and $b$ given $N$.

 By definition, the expected value equals:
\begin{equation}
  E(a | a + b = N) = \frac{\sum_{a=0}^\infty p(a | a + b = N) a}
                          {\sum_{a=0}^\infty p(a | a + b = N)}
  \label{eq:appab1}
\end{equation}
The denominator should be equal to 1, but if we keep it, we can apply the
equation also if $p$ is known except for a constant factor.

The prior expectations for $a$ and $b$ are:
\begin{equation}
  p_{\bar{a}}(a) = e^{-\bar{a}} \frac{\bar{a}^a}{a!} \hspace{2cm}
  p_{\bar{b}}(b) = e^{-\bar{b}} \frac{\bar{b}^b}{b!}
\end{equation}
After the measurement, we know that the first detector can only have produced
$a$ counts if the other one happened to contribute $N-a$ counts. Dropping some
constants this yields:
\begin{eqnarray}
  p(a | a + b = N) \sim e^{-\bar{a}} \frac{\bar{a}^a}{a!} \;\;
                        e^{-\bar{b}} \frac{\bar{b}^{N-a}}{(N-a)!}\\
    \sim \frac{\bar{a}^a}{a!} \frac{\bar{b}^{N-a}}{(N-a)!}
\end{eqnarray}
Applying \ref{eq:appab1} yields:
\begin{equation}
  E(a | a+b=N)
  = \frac{\sum_{a=0}^N \frac{\bar{a}^a}{a!} \frac{\bar{b}^{N-a}}{(N-a)!} a}
         {\sum_{a=0}^N \frac{\bar{a}^a}{a!} \frac{\bar{b}^{N-a}}{(N-a)!}}
\end{equation}
Let us first look at the denominator:
\begin{equation}
  \sum_{a=0}^N \frac{\bar{a}^a}{a!} \frac{\bar{b}^{N-a}}{(N-a)!}
  = \frac{(\bar{a} + \bar{b})^N}{N!}
\hspace{1cm} \mbox{since } \left( \begin{array}{cc} N\\a \end{array}\right)
   = \frac{N!}{a! (N-a)!}
\end{equation}
A bit more work needs to be done for the numerator:
\begin{eqnarray}
 \sum_{a=0}^N \frac{\bar{a}^a}{a!} \frac{\bar{b}^{N-a}}{(N-a)!} a
  & = & \sum_{a=1}^N \frac{\bar{a}^a}{a!} \frac{\bar{b}^{N-a}}{(N-a)!} a
    \hspace{1cm} \mbox{summation can start from 1}\\
  & = & \sum_{a=1}^N \frac{\bar{a}^a}{(a-1)!} \frac{\bar{b}^{N-a}}{(N-a)!}\\
  & = & \bar{a} \sum_{a=1}^N \frac{\bar{a}^{a-1}}{(a-1)!}
                             \frac{\bar{b}^{N-a}}{(N-a)!}\\
  & = & \bar{a} \sum_{a=0}^{N-1} \frac{\bar{a}^a}{a!} 
                                 \frac{\bar{b}^{N-1-a}}{(N-1-a)!}\\
  & = & \bar{a} \frac{(\bar{a} + \bar{b})^{N-1}}{(N-1)!}
\end{eqnarray}
Combining numerator and denominator results in:
\begin{eqnarray}
  E(a | a+b=N) & = & \bar{a} \frac{(\bar{a} + \bar{b})^{N-1}}{(N-1)!}
                     \frac{N!}{(\bar{a} + \bar{b})^N}\\
  & = & \bar{a} \frac{N}{\bar{a} + \bar{b}}
\end{eqnarray}

\newpage
\section{The convergence of the EM algorithm \label{app:em}}
%=============================================
This section explains why maximizing the likelihood of the complete variables
$L_x$ is equivalent to maximizing the likelihood $L$ of the observed variables
$Q$.

First some notations and definitions must be introduced. As in the text,
$\Lambda$ denotes the reconstruction, $Q$ the measured sinogram and $X$ the
complete variables. We want to maximize the logarithm of the likelihood
given the reconstruction:
\begin{equation}
  L(\Lambda) = \ln g(Q | \Lambda)
\end{equation}
The conditional likelihood of the complete variables $f(X | \Lambda)$  can be
written as:
\begin{equation}
  f(X | \Lambda) = k(X | Q, \Lambda) \; g(Q | \Lambda),
\end{equation}
where $k(X | Q, \Lambda)$ is the conditional likelihood of the complete
variables, given the reconstruction and the measurement. These definitions
immediately  imply that
\begin{equation}
  \ln f(X | \Lambda) = L(\Lambda) + \ln k(X | Q, \Lambda). \label{eq:appmlem_f}
\end{equation}

The objective function we construct during the E-step is defined as
\begin{equation}
  h(\Lambda' | \Lambda) = E\left[\ln f(X | \Lambda') | Q, \Lambda)\right],
      \label{eq:appmlem_h}
\end{equation}
which means that we write the log-likelihood of the complete variables as a
function of $\Lambda'$, and that we eliminate the unknown variables $X$ by
computing the expectation based on the current reconstruction $\Lambda$.
Combining (\ref{eq:appmlem_f}) and (\ref{eq:appmlem_h}) results in
\begin{equation}
  h(\Lambda' | \Lambda) = L(\Lambda') + E\left[\ln k(X | Q, \Lambda') | Q,\Lambda\right]
   \label{eq:appmlem_h2}
\end{equation}

Finally, we define a generalized EM (GEM) algorithm. This is a procedure which
computes a new reconstruction from the current one. The procedure will be
denoted as $M$. $M$ is a GEM-algorithm if
\begin{equation}
  h(M(\Lambda) | \Lambda) \geq h(\Lambda | \Lambda).
\end{equation}
This means that we want $M$ to increase $h$. We can be more demanding and
require that $M$ maximizes $h$; then we have a regular EM algorithm, such as
the MLEM algorithm of section \pref{sec:iterrecon}.

Now, from equation (\ref{eq:appmlem_h2}) we can compute what happens with $L$
if we apply a GEM-step to increase the value of $h$:
\begin{eqnarray}
  L(M(\Lambda)) - L(\Lambda) & = &
  h(M(\Lambda) | \Lambda) - h(\Lambda, \Lambda) \nonumber \\
  & &  + E\left[\ln k(X | Q, \Lambda) | Q,\Lambda\right] 
       - E\left[\ln k(X | Q, M(\Lambda)) | Q,\Lambda\right] \label{eq:appmlem_l}
\end{eqnarray}
Because $M$ is a GEM-algorithm, we already know that $h(M(\Lambda) | \Lambda) -
h(\Lambda, \Lambda)$ is positive. If we can also show that
\begin{equation}
  E\left[\ln k(X | Q, \Lambda) | Q,\Lambda\right] 
   - E\left[\ln k(X | Q, M(\Lambda)) | Q,\Lambda\right] \geq 0 \label{eq:appmlem_k}
\end{equation}
then we have proven that every GEM-step increases the likelihood $L$.\\[3mm]

By definition, we have
\begin{equation}
  E\left[\ln k(X | Q, \Lambda') | Q,\Lambda\right] =
     \int k(X | Q, \Lambda) \ln k(X | Q, \Lambda') dX.
\end{equation}
Therefore, the left hand side of equation (\ref{eq:appmlem_k}) can be
rewritten as
\begin{eqnarray}
& &  \int k(X | Q, \Lambda) \ln k(X | Q, \Lambda) dX 
  - \int k(X | Q, \Lambda) \ln k(X | Q, M(\Lambda)) dX \\
& = & \int k(X | Q, \Lambda)
        \ln \frac{k(X | Q, \Lambda)}{k(X | Q, M(\Lambda))} dX, 
        \label{eq:appmlem_k2}
\end{eqnarray}
with the additional requirement that $\int k(X | Q, \Lambda) dX = \int k(X |
Q, M(\Lambda)) dX = 1$. It turns out that (\ref{eq:appmlem_k2}) is always
positive, due to the convexity of $t \ln t$. We will have a look at that
now.\\[3mm]

Consider the function $\psi(t) = t \ln t$. It is only defined for $t >
0$. Here are its derivatives:
\begin{eqnarray}
  \psi'(t) = \frac{d \psi(t)}{dt} & = & 1 + \ln t\\
  \psi''(t) = \frac{d^2 \psi(t)}{dt^2} & = & \frac{1}{t} > 0
\end{eqnarray}
The second derivative is continuous and strictly positive ($t \ln t$ is a
convex function). Consequently, we can always find a value $u$ such that
\begin{equation}
  \psi(t) = \psi(1) + (t - 1) \psi'(1) + \frac{(t-1)^2}{2} \psi''(u) 
       \;\; \mbox{with} \;\; 0 < u \leq t.
\end{equation}
Because $\psi(1) = 0$ and $\psi'(1) = 1$, this becomes:
\begin{equation}
  \psi(t) = (t - 1) + \frac{(t-1)^2}{2} \psi''(u) 
       \;\; \mbox{with} \;\; 0 < u \leq t.
\end{equation}

Consider now an integral of the same form as in (\ref{eq:appmlem_k2}):
\begin{equation}
  \int f_1(x) \ln \frac{f_1(x)}{f_2(x)} dx \;\; \mbox{with} \;\;
  \int f_1(x) dx = \int f_2(x) dx = 1.
\end{equation}
We can rework it such that we can exploit the convexity of $t \ln t$:
\begin{eqnarray}
  & &  \int f_1(x) \ln \frac{f_1(x)}{f_2(x)} dx \nonumber\\
  & = & \int \frac{f_1(x)}{f_2(x)} \ln \left( \frac{f_1(x)}{f_2(x)} \right)
        f_2(x) dx \nonumber\\
  & = & \int \left[ \frac{f_1(x)}{f_2(x)} -1 + 
       \frac{1}{2}\left( \frac{f_1(x)}{f_2(x)} - 1\right)^2 
        \psi''(u(x)) \right]  f_2(x) dx \; \mbox{with} \; 0 < u(x) < \infty 
        \nonumber\\
  & = & \int \left( f_1(x) - f_2(x) \right) dx + 
        \frac{1}{2} \int \left( \frac{f_1(x)}{f_2(x)} - 1 2\right)^2
        \psi''(u(x)) f_2(x) dx \;\; \geq \;\; 0 \nonumber\\
  & & \mbox{q.e.d.} \nonumber
\end{eqnarray}

Now we have proved that the GEM-step increases $L$. It still remains
to be shown that the algorithm indeed converges towards the maximum
likelihood solution. If you want to know really everything about it, you
should read the paper by Dempster, Laird and Rubin, ``Maximum
likelihood from incomplete data via the EM algorithm'', {\em J R
Statist Soc} 1977; 39; 1-38.


\newpage
\section{Backprojection of a projection of a point source\label{app:bprojproj}}
%=========================
\subsection{2D parallel projection}
%----------------------------------
Consider a point source located in the center of the field of view.
The projection is a sinogram $q(s, \theta)$, given by
\begin{equation}
  q(s, \theta) = \delta(s),
\end{equation}
where $\delta(x)$ is the delta function: $\delta(x) = 1$ if $x = 0$, and
$\delta(x) = 0$ if $x \neq 0$.
The backprojection $b(x,y)$ is then given by
\begin{eqnarray}
  b(x,y) & = & \int_0^\pi q(x \cos\theta + y\sin\theta, \theta)
                d\theta \nonumber\\
         & = & \int_0^\pi \delta(x\cos\theta + y\sin\theta) d\theta
                \label{eq:app2bp1}
\end{eqnarray}
To proceed, the argument of the delta function must be changed. This
can be done with the following expression:
\begin{equation}
  \delta(f(x)) = \sum_1^N \frac{\delta(x_n)}{|f'(x_n)|},
\end{equation}
where $f'$ is the derivative of $f$, and $x_n, n=1 \ldots N$ are the
zeros of $f(x)$, i.e. $f(x_n) = 0$. A simple example is $\delta(ax) =
\delta(x) / |a|$. Applying this to (\ref{eq:app2bp1}) yields:
\begin{equation}
 b(x,y)  =  \int_0^\pi \frac{\delta(\theta - \theta_0)}
                              {|-x\sin\theta_0 + y\cos\theta_0|} d\theta,
\end{equation}
where $\theta_0$ is such that $x\cos\theta_0 + y\sin\theta_0 =
0$. This is satisfied if
\begin{displaymath}
  \cos\theta_0 = \pm \frac{y}{\sqrt{x^2+y^2}} 
       \;\;\;\mbox{and}\;\;\;
  \sin\theta_0 = \mp \frac{x}{\sqrt{x^2+y^2}},
\end{displaymath}
which results in
\begin{equation}
  b(x,y) = \frac{1}{\sqrt{x^2 + y^2}}.
\end{equation}

\subsection{2D parallel projection with TOF}
%-------------------------------------------
Consider again the projection of a point source in the center of the
field of view. With time-of-flight, the parallel projection along
angle $\theta$ can be considered as a Gaussian blurring along lines
with angle $\theta$. That gives for the TOF-sinogram $q(x,y,\theta)$:
\begin{equation}
 q(x,y,\theta) = \frac{1}{\sqrt{2\pi}\sigma}
               \exp(-\frac{x^2+y^2}{2 \sigma^2}) \;
              \delta(x\cos\theta + y\sin\theta),
\end{equation}
where $\sigma$ represents the uncertainty of the TOF-measurement. The
corresponding backprojection operator is obtained by applying the same
blurring to the projections, followed by integration over
$\theta$. The convolution of a Gaussian with itself is equivalent to
multiplying its standard deviation with $\sqrt{2}$ (see appendix
\pref{app:convol2gauss}). One finds
\begin{eqnarray}
  b(x,y) & = & \int_0^\pi \frac{1}{2\sqrt{\pi}\sigma}
               \exp(-\frac{x^2+y^2}{4 \sigma^2}) \;
              \delta(x\cos\theta + y\sin\theta) d\theta\\
 &=& \frac{1}{2\sqrt{\pi}\sigma \sqrt{x^2 + y^2}}
               \exp(-\frac{x^2+y^2}{4 \sigma^2}),
\end{eqnarray}
which is equivalent to equation (\pref{eq:TOFpsf}).